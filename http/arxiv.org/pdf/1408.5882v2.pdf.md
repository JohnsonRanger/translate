# Convolutional Neural Networks for Sentence Classification
# 使用卷积神经网络实现句子分类

Yoon Kim New York University yhk255@nyu.edu


## Abstract

We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks.  We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.

这是我们为了完成句子级别的文本分类任务，做的一系列基于预先训练的词向量而训练的卷积神经网络（CNN）的实验的报告。我们展示了一个做了少量超参优化，使用静态词向量的简单的CNN在多个基准测试上都取得了优异的成绩。通过对特定任务的学习的向量优化可以进一步提高性能。我们还给出了一个提案，通过对架构的简单调整，我们可以同时利用基于特定任务的向量和静态向量。我们这里讨论的CNN模型提高了7个任务中的4个的工艺水平，其中包括语义分析和问题分类

## 1 Introduction

Deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012) and speech recognition (Graves et al., 2013) in recent years. Within natural language processing, much of the work with deep learning methods has involved learning word vector representations through neural language models (Bengio et al., 2003; Yih et al., 2011; Mikolov et al., 2013) and performing composition over the learned word vectors for classification (Collobert et al., 2011).  Word vectors, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabulary size) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimensions.  In such dense representations, semantically close words are likewise close—in euclidean or cosine distance—in the lower dimensional vector space.

最近几年深度学习模型在计算机视觉(Krizhevsky et al., 2012)和语音识别领域取得了举世瞩目的成就。在自然语言处理方面，深度学习的大多数方法都通过自然语言模型来学习词向量表示（Bengio et al., 2003; Yih et al., 2011; Mikolov et al., 2013），在学习的词向量的基础上进行组合来解决分类问题(Collobert et al., 2011)。词向量，即通过一个隐藏层将词从一个稀疏的，1到V(V即词典大小)的的向量编码为一个低维向量，本质上是在各维度上编码语义特征的特征提取问题。在这个低维高密的表示里，语义相近的词的欧氏距离或余弦距离比较接近。


Convolutional neural networks (CNN) utilize layers with convolving filters that are applied to local features (LeCun et al., 1998). Originally invented for computer vision, CNN models have subsequently been shown to be effective for NLP and have achieved excellent results in semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalchbrenner et al., 2014), and other traditional NLP tasks (Collobert et al., 2011).

卷积神经网络（CNN）利用带有卷积过滤器的层进行特征提取(LeCun et al., 1998)，发明的初衷是用于计算机视觉，CNN模型随后在NLP展示了它的高效，并且在语义分析(Yih et al., 2014)，查询检索(Shen et al., 2014)，语义模型 (Kalchbrenner et al., 2014)以及其他传统的NLP任务(Collobert et al., 2011)中取得了非凡的成就。

In the present work, we train a simple CNN with one layer of convolution on top of word vectors obtained from an unsupervised neural language model. These vectors were trained by Mikolov et al. (2013) on 100 billion words of Google News, and are publicly available.  We initially keep the word vectors static and learn only the other parameters of the model. Despite little tuning of hyperparameters, this simple model achieves excellent results on multiple benchmarks, suggesting that the pre-trained vectors are ‘universal’ feature extractors that can be utilized for various classification tasks. Learning task-specific vectors through fine-tuning results in further improvements. We finally describe a simple modification to the architecture to allow for the use of both pre-trained and task-specific vectors by having multiple channels.

在当前的工作中，我们在一个简单的通过无人监督自然语言模型获取的词向量的基础上，训练一个了单层卷积层的CNN。这里的词向量是Mikolov et al. (2013) 基于谷歌新闻里的1000亿词训练的公开结果。我们首先将这些词向量作为静态不变的，只学习模型的其他参数。尽管只是调整了很少的超参，这个简单的模型在多项基准测试里都取得了非凡的成绩。这表明了这些预先训练的向量是可以在各种分类任务里利用的通用的特征。学习并调整特定任务的向量可以进一步改进结果。最后，我们描述了一个对架构的简单修改，通过多通道，可以同时利用预先训练的向量和特定任务的向量。

Our work is philosophically similar to Razavian et al. (2014) which showed that for image classification, feature extractors obtained from a pretrained deep learning model perform well on a variety of tasks—including tasks that are very different from the original task for which the feature extractors were trained.

我们的工作原理和Razavian et al. (2014)展示的图片分类的原理类似，通过预先训练的深度学习模型获取的特征在多种任务中都表现突出，包括在那些和我们进行训练来提取特征的原始任务非常不同的任务。


## 2 Model
## 2 模型

![figure 1](1408.5882v2.pdf/images/1.png)

The model architecture, shown in figure 1, is a slight variant of the CNN architecture of Collobert et al. (2011). Let ![](http://latex.codecogs.com/gif.latex?x_%7B_%7Bi%7D%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bk%7D) be the `k`-dimensional word vector corresponding to the `i`-th word in the sentence. A sentence of length `n` (padded where necessary) is represented as

如图1中所展示的模型架构，是 Collobert et al. (2011)的CNN架构的一个轻微变体。假设![](http://latex.codecogs.com/gif.latex?x_%7B_%7Bi%7D%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bk%7D) 是句子中第`i`个词的`k`维词向量。一个长度为`n`(padded where necessary) 的句子表示为：

![](http://latex.codecogs.com/gif.latex?x_%7B1%3An%7D%3Dx_1%5Cbigoplus%20x_2%5Cbigoplus...%5Cbigoplusx_n)` `` `` `(1)

where ⊕ is the concatenation operator. In general,let ![](http://latex.codecogs.com/gif.latex?x_%7Bi%3Ai&plus;j%7D)r efer to the concatenation of words ![](http://latex.codecogs.com/gif.latex?x_i%2Cx_%7Bi&plus;1%7D%2C...%2Cx_%7Bi&plus;j%7D) . A convolution operation involves a filter ![](http://latex.codecogs.com/gif.latex?w%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bhk%7D) which is applied to a window of `h` words to produce a new feature. For example, a feature ![](http://latex.codecogs.com/gif.latex?c_i) is generated from a window of words ![](http://latex.codecogs.com/gif.latex?x_%7Bi%3Ai&plus;h-1%7D) by

这里的⊕ 是串联符。一般的，假设 [](http://latex.codecogs.com/gif.latex?x_%7Bi%3Ai&plus;j%7D) 表示单词  ![](http://latex.codecogs.com/gif.latex?x_i%2Cx_%7Bi&plus;1%7D%2C...%2Cx_%7Bi&plus;j%7D) 的串联。对大小为`h`的窗口里的词进行卷积运算 ![](http://latex.codecogs.com/gif.latex?w%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bhk%7D) 生成一个新的特征。 比如，![](http://latex.codecogs.com/gif.latex?x_%7Bi%3Ai&plus;h-1%7D) 窗口里的词，通过下面的运算生成的新的特征 ![](http://latex.codecogs.com/gif.latex?c_i) ：

![](http://latex.codecogs.com/gif.latex?c_i%3Df%28w%5Ccdot%20x_%7Bi%3Ai&plus;h-1%7D&plus;b%29)` `` `` `(2)

Here ![](http://latex.codecogs.com/gif.latex?b%20%5Cin%20%5Cmathbb%7BR%7D) is a bias term and `f` is a non-linear
function such as the hyperbolic tangent. This filter
is applied to each possible window of words in the
sentence ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7BBmatrix%7D%20x_%7B1%3Ah%7D%2Cx_%7B2%3Ah&plus;1%7D%2C...%2Cx_%7Bn-h&plus;1%3An%7D%20%5Cend%7BBmatrix%7D) to produce a `feature map`

这里的 ![](http://latex.codecogs.com/gif.latex?b%20%5Cin%20%5Cmathbb%7BR%7D) 是偏差项，`f` 是一个非线性函数，比如双曲正切。将这个过滤器应用于句子里的所有的窗口 ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7BBmatrix%7D%20x_%7B1%3Ah%7D%2Cx_%7B2%3Ah&plus;1%7D%2C...%2Cx_%7Bn-h&plus;1%3An%7D%20%5Cend%7BBmatrix%7D) ，就产生了一个 `feature map`

![](http://latex.codecogs.com/gif.latex?c%3D%5Cbegin%7Bbmatrix%7D%20c_1%2Cc_2%2C...%2Cc_%7Bn-h&plus;1%7D%20%5Cend%7Bbmatrix%7D)` `` `` `(3)

with ![](http://latex.codecogs.com/gif.latex?c%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn-h&plus;1%7D) We then apply a max-overtime pooling operation (Collobert et al., 2011) over the feature map and take the maximum value ![](http://latex.codecogs.com/gif.latex?%5Chat%7Bc%7D%3Dmax%5Cbegin%7BBmatrix%7D%20c%20%5Cend%7BBmatrix%7D) as the feature corresponding to this particular filter. The idea is to capture the most important feature—one with the highest value—for each feature map. This pooling scheme naturally deals with variable sentence lengths.

有了 ![](http://latex.codecogs.com/gif.latex?c%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn-h&plus;1%7D)  ，我们随后将max-overtime pooling operation (Collobert et al., 2011)应用到这个feature map上，取最大值 ![](http://latex.codecogs.com/gif.latex?%5Chat%7Bc%7D%3Dmax%5Cbegin%7BBmatrix%7D%20c%20%5Cend%7BBmatrix%7D) 作为这个过滤器的特征。其思想是选取每个feature map里的值最大的，也就是最重要的特征。这个方法很自然的结局了句子长度不一致的问题。

We have described the process by which `one` feature is extracted from `one` filter. The model uses multiple filters (with varying window sizes) to obtain multiple features. These features form the penultimate layer and are passed to a fully connected softmax layer whose output is the probability distribution over labels.

我们刚才描述了如何从`一个`过滤器，提取出`一个`特征，这个模型使用多个过滤器（通过变化窗口大小)来获取多个特征。这些特征组成了倒数第二层，传给了一个全链接的softmax层，softmax层的输出是不同标签的分布概率。

In one of the model variants, we experiment with having two ‘channels’ of word vectors—one that is kept static throughout training and one that is fine-tuned via backpropagation (section 3.2) In the multichannel architecture, illustrated in figure 1, each filter is applied to both channels and the results are added to calculate ![](http://latex.codecogs.com/gif.latex?c_i) in equation (2). The model is otherwise equivalent to the single channel architecture.

在这个模型的一个变体中，我们实验了通过两个`通道`的词向量- 一个在训练中保持为静态不变，另外一个通过反向传播(3.2节)来优化调整。在入图1所示的多通道架构里，没个过滤器都被应用于两个通道，然后将结果相加结果作为 ![](http://latex.codecogs.com/gif.latex?c_i) 。模型的其他部分和单通道模型一致。

### 2.1 Regularization
### 2.1 正则化

For regularization we employ dropout on the penultimate layer with a constraint on ![](http://latex.codecogs.com/gif.latex?l_2-norms) of the weight vectors (Hinton et al., 2012). Dropout prevents co-adaptation of hidden units by randomly dropping out—i.e., setting to zero—a proportion `p` of the hidden units during fowardbackpropagation. That is, given the penultimate layer ![](http://latex.codecogs.com/gif.latex?z%3D%5Cbegin%7Bbmatrix%7D%20%5Chat%7Bc%7D_%7B1%7D%2C%5Chat%7Bc%7D_%7B2%7D%2C...%5Chat%7Bc%7D_%7Bm%7D%20%5Cend%7Bbmatrix%7D) (note that here we have `m` filters), instead of using

为了正则化，我们在倒数第二层使用一个带有 ![](http://latex.codecogs.com/gif.latex?l_2-norms)  约束的淘汰器。淘汰器通过在前向反向传播随机丢弃比例为`p`的隐藏节点输出(比如，设置为0）来阻止隐藏单元的co-adaptation。即，在倒数第二层里 ![](http://latex.codecogs.com/gif.latex?z%3D%5Cbegin%7Bbmatrix%7D%20%5Chat%7Bc%7D_%7B1%7D%2C%5Chat%7Bc%7D_%7B2%7D%2C...%5Chat%7Bc%7D_%7Bm%7D%20%5Cend%7Bbmatrix%7D) （我们有`m`个过滤器），我们不使用如下的公式：

![](http://latex.codecogs.com/gif.latex?y%3Dw%5Ccdot%20z&plus;b)` `` `` `(4)

for output unit y in forward propagation, dropout uses 
取而代之，我们对前向传播的输出单元y，使用如下公式丢弃

![](http://latex.codecogs.com/gif.latex?y%3Dw%5Ccdot%20%28z%5Ccirc%20r%29&plus;b)` `` `` `(5)

where ![](http://latex.codecogs.com/gif.latex?%5Ccirc) is the element-wise multiplication operator and ![](http://latex.codecogs.com/gif.latex?r%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%7D) is a ‘masking’ vector of Bernoulli random variables with probability `p` of being 1. Gradients are backpropagated only through the unmasked units. At test time, the learned weight vectors are scaled by `p` such that ![](http://latex.codecogs.com/gif.latex?%5Chat%7Bw%7D%3Dpw) and ![](http://latex.codecogs.com/gif.latex?%5Chat%7Bw%7D) is used (without dropout) to score unseen sentences. We additionally constrain ![](http://latex.codecogs.com/gif.latex?l_2-norms) of the weight vectors by rescaling `w` to have ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7BVmatrix%7D%20w%20%5Cend%7BVmatrix%7D_2%3Ds) whenever ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7BVmatrix%7D%20w%20%5Cend%7BVmatrix%7D_2%3Es) after a gradient descent step.

这里的 ![](http://latex.codecogs.com/gif.latex?%5Ccirc) 是一个点乘(内积)符号 ![](http://latex.codecogs.com/gif.latex?r%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%7D) 是一个`屏蔽`向量，它遵从伯努利随机分布，有概率`p` 的可能为1。梯度只能通过未屏蔽的单元后向传播。在测试阶段，学习的权重向量通过`p`调整，即  ![](http://latex.codecogs.com/gif.latex?%5Chat%7Bw%7D%3Dpw) ，这里的 ![](http://latex.codecogs.com/gif.latex?%5Chat%7Bw%7D) ，不被丢弃，用来给新的句子打分。我们还有个附加的约束权重向量的 ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7BVmatrix%7D%20w%20%5Cend%7BVmatrix%7D_2%3Ds)，在每词梯度下降后，如果 ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7BVmatrix%7D%20w%20%5Cend%7BVmatrix%7D_2%3Es) ，则调整`w` 使得 ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7BVmatrix%7D%20w%20%5Cend%7BVmatrix%7D_2%3Ds)


## 3 Datasets and Experimental Setup
## 3 数据集及实验步骤

We test our model on various benchmarks. Summary statistics of the datasets are in table 1.
我们在多种基准测试上测试了我们的模型。数据集的统计数据见表1

Data         | c            | l            | N            | ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7Bvmatrix%7D%20V%20%5Cend%7Bvmatrix%7D)          | ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7Bvmatrix%7D%20V_%7Bpre%7D%20%5Cend%7Bvmatrix%7D)         | Test
------ | ------ | ------ | ------ | ------ | ------ | ------
MR     |   2    |   20   |  10662 | 18765  |  16448 |CV
SST-1  |  5     |   18   |  11855 | 17836  |  16262 |2210
SST-2  |  2     |   19   |  9613  | 16185  |  14838 |1821
Subj   |  2     |   23   |  10000 | 21323  |  17913 |CV
TREC   |  6     |   10   |  5952  | 9592   |  9125  |500
CR     |  2     |   19   |  3775  | 5340   |  5046  |CV
MPQA   |  2     |   3    |  10606 | 6246   |  6083  |CV

Table 1: Summary statistics for the datasets after tokenization. `c`: Number of target classes. `l`: Average sentence length. `N`: Dataset size. ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7Bvmatrix%7D%20V%20%5Cend%7Bvmatrix%7D) : Vocabulary size. ![](http://latex.codecogs.com/gif.latex?%5Cbegin%7Bvmatrix%7D%20V_%7Bpre%7D%20%5Cend%7Bvmatrix%7D) : Number of words present in the set of pre-trained word vectors. `Test`: Test set size (CV means there was no standard train/test split and thus 10-fold CV was used).

表1： 数据集标记后的汇总数据。`c`: 目标分类个数。`l`: 平均句子长度。`N`:数据集大小。![](http://latex.codecogs.com/gif.latex?%5Cbegin%7Bvmatrix%7D%20V%20%5Cend%7Bvmatrix%7D): 词典大小。![](http://latex.codecogs.com/gif.latex?%5Cbegin%7Bvmatrix%7D%20V_%7Bpre%7D%20%5Cend%7Bvmatrix%7D) ：在预先训练的词向量里出现的词。`Test`: 测试数据集大小（CV表示没有标准的训练/测试数据的划分，而是使用十折交叉验证（10-fold cross validation))


 * `MR`: Movie reviews with one sentence per review. Classification involves detecting positive/negative reviews (Pang and Lee, 2005)
 * `SST-1`: Stanford Sentiment Treebank—an extension of MR but with train/dev/test splits provided and fine-grained labels (very positive, positive, neutral, negative, very negative), re-labeled by Socher et al. (2013).
 * `SST-2`: Same as SST-1 but with neutral reviews removed and binary labels
 * `Subj`: Subjectivity dataset where the task is to classify a sentence as being subjective or objective (Pang and Lee, 2004)
 * `TREC`: TREC question dataset—task involves classifying a question into 6 question types (whether the question is about person, location, numeric information, etc.) (Li and Roth, 2002).
 * `CR`: Customer reviews of various products (cameras, MP3s etc.). Task is to predict positive/negative reviews (Hu and Liu, 2004)
 * `MPQA`: Opinion polarity detection subtask of the MPQA dataset (Wiebe et al., 2005)

 * `MR`: 电影评论，每个评论一句话。分类包含检测正面/负面的评论 (Pang and Lee, 2005)
 * `SST-1`: 斯坦福情感树库,在`MR`的基础上做了训练/开发/测试的区分，并且Socher et al.给出了细粒度的标签(2013).
 * `SST-2`: 在`SST-1`的基础上去除了中性的评价，给出了二进制的标签。
 * `Subj`: 主观数据集，目标是将句子按照主观和客观进行分类 (Pang and Lee, 2004)
 * `TREC`: TREC问题数据集—目标是将问题归为6中问题类型（问题是关于人，地点，数字信息等） (Li and Roth, 2002).
 * `CR`: 对各种产品（比如相机，MP3等）的客户评价。目标是区分是正面/负面评价 (Hu and Liu, 2004)
 * `MPQA`: MPQA数据集中的意见极性检测 (Wiebe et al., 2005)
 
## 3.1 Hyperparameters and Training

## 3.1 超参和训练

For all datasets we use: rectified linear units, filter windows `(h)` of 3, 4, 5 with 100 feature maps each, dropout rate `(p)` of 0.5, ![](http://latex.codecogs.com/gif.latex?l_2) constraint `(s)` of 3, and mini-batch size of 50. These values were chosen via a grid search on the SST-2 dev set.

对于我们的所有数据集：整流线性单元，使用大小为3,4,5(`h`)的窗口，每个窗口对100个特征map，使用0.5(`p`)的淘汰率,![](http://latex.codecogs.com/gif.latex?l_2) 约束为3（`s`）,迷你批次大小设置为50.这些值是通过对`SST-2`开发数据集进行网格搜索获取的。

We do not otherwise perform any datasetspecific tuning other than early stopping on dev sets. For datasets without a standard dev set we randomly select 10% of the training data as the dev set. Training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).

我们除了在开发数据集上使用提前停止法（early stopping)以外，不做其他针对某个数据集的优化调整。对于那些没有开发数据集的数据集，我们随机挑选10%的训练数据作为开发数据集。通过在打乱的迷你批次上的基于Adadelta update rule的随机梯度下降来进行训练。

## 3.2 Pre-trained Word Vectors

## 3.2 预先训练的词向量

Initializing word vectors with those obtained from an unsupervised neural language model is a popular method to improve performance in the absence of a large supervised training set (Collobert et al., 2011; Socher et al., 2011; Iyyer et al., 2014). We use the publicly available `word2vec` vectors that were trained on 100 billion words from Google News. The vectors have dimensionality of 300 and were trained using the continuous bag-of-words architecture (Mikolov et al., 2013). Words not present in the set of pre-trained words are initialized randomly.

在没有大规模训练数据的情况下，使用通过非监督自然语言模型获取的词向量来初始化，是一个提高性能的非常流行的方法。Collobert et al., 2011; Socher et al., 2011; Iyyer et al., 2014) 。我们使用公开的通过10亿谷歌新闻词汇训练的`word2vec`向量。这个向量包含300个维度，是通过持续词袋架构进行训练的。在预处理的词中没有的词，则使用随机值初始化。


## 3.3 Model Variations

## 3.3 模型变体

We experiment with several variants of the model.

我们实验了模型的多种变体

 * `CNN-rand`: Our baseline model where all words are randomly initialized and then modified during training.
 * `CNN-static`: A model with pre-trained vectors from word2vec. All words— including the unknown ones that are randomly initialized—are kept static and only the other parameters of the model are learned.
 * `CNN-non-static`: Same as above but the pretrained  vectors are fine-tuned for each task.
 * `CNN-multichannel`: A model with two sets of word vectors. Each set of vectors is treated as a ‘channel’ and each filter is applied to both channels, but gradients are backpropagated only through one of the channels. Hence the model is able to fine-tune one set of vectors while keeping the other static. Both channels are initialized with word2vec.

 * `CNN-rand`: 我们的基础模型，所有词都随机初始化，然后在训练中修改。
 * `CNN-static`: 使用了word2vec预先训练词的模型。所有词-包括因为在预先训练的词库中不存在的而随机初始化的词-都保持不变，只学习模型的其他参数。
 * `CNN-non-static`: 和上面的模型类似，不同个是预先训练的向量是为每个task调优的
 * `CNN-multichannel`: 包含两个向量集的模型。每个向量集都被当作一个通道，每个过滤器都同时应用两个通道，但是梯度只通过一个通道反向传播。因此这个模型可以在保持一个向量集不变的情况下优化另一个向量集。两个向量集都通过word2vec初始化。
 
In order to disentangle the effect of the above variations versus other random factors, we eliminate other sources of randomness—CV-fold assignment, initialization of unknown word vectors, initialization of CNN parameters—by keeping them uniform within each dataset.

为了排除其他随机因素对不同变体的影响，我们在不同的数据集里使用相同的随机交叉验证，未知词的初始化，CNN的初始参数等。
 
## 4 Results and Discussion

## 4 结果和讨论

Model                                |   MR   |SST-1   |SST-2   |Subj    | TREC   | CR    | MPQA
------------------------------------ | ------ | ------ | ------ | ------ | ------ | ------| ------
CNN-rand                             | 76.1   |  45.0  |  82.7  |  89.6  |  91.2  | 79.8  | 83.4
CNN-static                           | 81.0   |  45.5  |  86.8  |  93.0  |  92.8  | 84.7  | 89.6
CNN-non-static                       | 81.5   |  48.0  |  87.2  |  93.4  |  93.6  | 84.3  | 89.5
CNN-multichannel                     | 81.1   |  47.4  |  88.1  |  93.2  |  92.2  | 85.0  | 89.4
RAE (Socher et al., 2011)            | 77.7   |  43.2  |  82.4  |  −     |  −     | −     | 86.4
MV-RNN (Socher et al., 2012)         | 79.0   |  44.4  |  82.9  |  −     |  −     | −     | −
RNTN (Socher et al., 2013)           | −      |  45.7  |  85.4  |  −     |  −     | −     | −
DCNN (Kalchbrenner et al., 2014)     | −      |  48.5  |  86.8  |  −     |  93.0  | −     | −
Paragraph-Vec (Le and Mikolov, 2014) | −      |  48.7  |  87.8  |  −     |  −     | −     | −
CCAE (Hermann and Blunsom, 2013)     | 77.8   |  −     |  −     |  −     |  −     | −     | 87.2
Sent-Parser (Dong et al., 2014)      | 79.5   |  −     |  −     |  −     |  −     | −     | 86.3
NBSVM (Wang and Manning, 2012)       | 79.4   |  −     |  −     |  93.2  |  −     | 81.8  | 86.3
MNB (Wang and Manning, 2012)         | 79.0   |  −     |  −     |  93.6  |  −     | 80.0  | 86.3
G-Dropout (Wang and Manning, 2013)   | 79.0   |  −     |  −     |  93.4  |  −     | 82.1  | 86.1
F-Dropout (Wang and Manning, 2013)   | 79.1   |  −     |  −     |  93.6  |  −     | 81.9  | 86.3
Tree-CRF (Nakagawa et al., 2010)     | 77.3   |  −     |  −     |  −     |  −     | 81.4  | 86.1
CRF-PR (Yang and Cardie, 2014)       | −      |  −     |  −     |  −     |  −     | 82.7  | −
SVMS (Silva et al., 2011)            | −      |  −     |  −     |  −     |  95.0  | −     | −

Table 2: Results of our CNN models against other methods. `RAE`: Recursive Autoencoders with pre-trained word vectors from Wikipedia (Socher et al., 2011). `MV-RNN`: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). `RNTN`: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). `DCNN`: Dynamic Convolutional Neural Network with k-max pooling (Kalchbrenner et al., 2014). `Paragraph-Vec`: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). `CCAE`: Combinatorial Category Autoencoders with combinatorial category grammar operators (Hermann and Blunsom, 2013). `Sent-Parser`: Sentiment analysis-specific parser (Dong et al., 2014). `NBSVM`, `MNB`: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). `G-Dropout`, `F-Dropout`: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). `Tree-CRF`: Dependency tree with Conditional Random Fields (Nakagawa et al., 2010). `CRF-PR`: Conditional Random Fields with Posterior Regularization (Yang and Cardie, 2014). `SVM`S: SVM with uni-bi-trigrams, wh word, head word, POS, parser, hypernyms, and 60 hand-coded rules as features from Silva et al. (2011).

Table 2: 我们的CNN模型和其他方法的结果对比。
* `RAE`: 使用Wikipedia预先训练的词做第归自动编码 (Socher et al., 2011).。
* `MV-RNN`: 使用分析树的矩阵向量第归神经网络(Socher et al., 2012).
* `RNTN`: 使用基于张量的特征函数和分析树的第归神经张量网络(Socher et al., 2013).
* `DCNN`: 使用k-max池的动态卷积神将网络(Kalchbrenner et al., 2014).
* `Paragraph-Vec`: 在段落向量上的逻辑回归(Le and Mikolov, 2014). 
* `CCAE`: 使用组合目录语法操作符的组合目录自编码(Hermann and Blunsom, 2013).
* `Sent-Parser`:情感分析 (Dong et al., 2014)
* `NBSVM`,`MNB`: Wang、Manning等的朴素贝叶斯支持向量机和使用uni-bigrams的多项贝叶斯(2012).
* `G-Dropout`, `F-Dropout`: 高斯丢弃和快速丢弃 Wang and Manning (2013).
* `Tree-CRF` 有条件随机域的依赖关系树(Nakagawa et al., 2010).
* `CRF-PR`: 后正则化条件随机场 (Yang and Cardie, 2014).
* `SVM`S: 支持向量机和单双卦、疑问词、头词、POS、语法分析器，上位词以及60个手动编码规则作为特征 Silva et al. (2011).

Results of our models against other methods are listed in table 2. Our baseline model with all randomly initialized words (CNN-rand) does not perform well on its own. While we had expected performance gains through the use of pre-trained vectors, we were surprised at the magnitude of the gains. Even a simple model with static vectors (CNN-static) performs remarkably well, giving competitive results against the more sophisticated deep learning models that utilize complex pooling schemes (Kalchbrenner et al., 2014) or require parse trees to be computed beforehand (Socheret al., 2013). These results suggest that the pretrained vectors are good, `universal` feature extractors and can be utilized across datasets. Finetuning the pre-trained vectors for each task gives still further improvements (CNN-non-static).

我们的模型和其他方法的结果在表2中列出。我们的基础模型，所有的词都随机初始化(CNN-rand)表现并不突出。所以，我们希望通过使用预先训练的词向量来提高性能。结果提升的幅度大大超出了预期。甚至使用静态词向量的简单模型也由显著的提升，足以和一些更复杂的，利用了预先准备好的复杂的池化schemes(Kalchbrenner et al., 2014)或分析树(Socheret al., 2013)的深度学习模型竞争。结果证明了预先训练的词向量非常棒，`通用` 的特征提取其可以在不同的数据集中使用。对每个任务优化的预先训练的词向量可以进一步提升效果(CNN-non-static)。



### 4.1 Multichannel vs. Single Channel Models

We had initially hoped that the multichannel architecture would prevent overfitting (by ensuring that the learned vectors do not deviate too far from the original values) and thus work better than the single channel model, especially on smaller datasets. The results, however, are mixed, and further work on regularizing the fine-tuning process is warranted. For instance, instead of using an additional channel for the non-static portion, one could maintain a single channel but employ extra dimensions that are allowed to be modified during training.


### 4.2 Static vs. Non-static Representations


<table>
 <tr><td rowspan="2"></td><td colspan="2">Most Similar Words for</td></tr>
 <tr><td>Static Channel </td><td>Non-static Channel</td></tr>
 <tr><td rowspan="4">bad</td><td>good</td><td>terrible</td></tr>
 <tr>                        <td>terrible</td><td>horrible</td></tr>
 <tr>                        <td>horrible</td><td>lousy</td></tr>
 <tr>                        <td>lousy</td><td>stupid</td></tr>
 <tr><td rowspan="4">good</td><td>great</td><td>nice</td></tr>
 <tr>                         <td>bad</td><td>decent</td></tr>
 <tr>                         <td>terrific</td><td>solid</td></tr>
 <tr>                         <td>decent</td><td>terrific</td></tr>
 <tr><td rowspan="4">n’t</td><td>os</td><td>not</td></tr>
 <tr>                         <td>ca</td><td>never</td></tr>
 <tr>                         <td>ireland</td><td>nothing</td></tr>
 <tr>                         <td>wo</td><td>neither</td></tr>
 <tr><td rowspan="4">!</td>   <td>2,500</td><td>2,500</td></tr>
 <tr>                         <td>entire</td><td>lush</td></tr>
 <tr>                         <td>jez</td><td>beautiful</td></tr>
 <tr>                         <td>changer</td><td>terrific</td></tr>
 <tr><td rowspan="4">,</td>   <td>decasia</td><td>but</td></tr>
 <tr>                         <td>abysmally</td><td>dragon</td></tr>
 <tr>                         <td>demise</td><td>a</td></tr>
 <tr>                         <td>valiant</td><td>and</td></tr>
</table>

Table 3: Top 4 neighboring words—based on cosine similarity—for vectors in the static channel (left) and finetuned vectors in the non-static channel (right) from the multichannel model on the SST-2 dataset after training.


As is the case with the single channel non-static model, the multichannel model is able to fine-tune the non-static channel to make it more specific to the task-at-hand. For example, `good` is most similar to `bad` in `word2vec`, presumably because they are (almost) syntactically equivalent. But for vectors in the non-static channel that were finetuned on the SST-2 dataset, this is no longer the case (table 3). Similarly, `good` is arguably closer to `nice` than it is to `great` for expressing sentiment, and this is indeed reflected in the learned vectors.  For (randomly initialized) tokens not in the set of pre-trained vectors, fine-tuning allows them to learn more meaningful representations: the network learns that exclamation marks are associated with effusive expres

### 4.3 Further Observations

We report on some further experiments and observations:
 * Kalchbrenner et al. (2014) report much worse results with a CNN that has essentially the same architecture as our single channel model. For example, their Max-TDNN (Time Delay Neural Network) with randomly initialized words obtains 37.4% on the SST-1 dataset, compared to 45.0% for our model.  We attribute such discrepancy to our CNN having much more capacity (multiple filter widths and feature maps).
 * Dropout proved to be such a good regularizer that it was fine to use a larger than necessary network and simply let dropout regularize it.  Dropout consistently added 2%–4% relative performance.
 * When randomly initializing words not in `word2vec`, we obtained slight improvements by sampling each dimension from U[−a, a] where a was chosen such that the randomly initialized vectors have the same variance as the pre-trained ones. It would be interesting to see if employing more sophisticated methods to mirror the distribution of pre-trained vectors in the initialization process gives further improvements.
 * We briefly experimented with another set of publicly available word vectors trained by Collobert et al. (2011) on Wikipedia,8 and found that `word2vec` gave far superior performance.  It is not clear whether this is due to Mikolov et al. (2013)’s architecture or the 100 billion word Google News dataset.
 * Adadelta (Zeiler, 2012) gave similar results to Adagrad (Duchi et al., 2011) but required fewer epochs.

## 5 Conclusion

In the present work we have described a series of experiments with convolutional neural networks built on top of word2vec. Despite little tuning of hyperparameters, a simple CNN with one layer of convolution performs remarkably well. Our results add to the well-established evidence that unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP.

## Acknowledgments

We would like to thank Yann LeCun and the anonymous reviewers for their helpful feedback and suggestions.

## References

Y. Bengio, R. Ducharme, P. Vincent. 2003. Neural Probabilitistic Language Model. Journal of Machine Learning Research 3:1137–1155.

R. Collobert, J. Weston, L. Bottou, M. Karlen, K.  Kavukcuglu, P. Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research 12:2493–2537.

J. Duchi, E. Hazan, Y. Singer. 2011 Adaptive subgradient methods for online learning and stochastic optimization.  Journal of Machine Learning Research, 12:2121–2159.

L. Dong, F. Wei, S. Liu, M. Zhou, K. Xu. 2014. A Statistical Parsing Framework for Sentiment Classi- fication. CoRR, abs/1401.6330.

A. Graves, A. Mohamed, G. Hinton. 2013. Speech recognition with deep recurrent neural networks. In Proceedings of ICASSP 2013.

G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors.  CoRR, abs/1207.0580.

K. Hermann, P. Blunsom. 2013. The Role of Syntax in Vector Space Models of Compositional Semantics.  In Proceedings of ACL 2013.

M. Hu, B. Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of ACM SIGKDD 2004.

M. Iyyer, P. Enns, J. Boyd-Graber, P. Resnik 2014.  Political Ideology Detection Using Recursive Neural Networks. In Proceedings of ACL 2014.
                                                                                                                                                                        
N. Kalchbrenner, E. Grefenstette, P. Blunsom. 2014. A Convolutional Neural Network for Modelling Sentences.  In Proceedings of ACL 2014.                                
                                                                                                                                                                        
A. Krizhevsky, I. Sutskever, G. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Proceedings of NIPS 2012.                             

Q. Le, T. Mikolov. 2014. Distributed Represenations of Sentences and Documents. In Proceedings of ICML 2014.

Y. LeCun, L. Bottou, Y. Bengio, P. Haffner. 1998.  Gradient-based learning applied to document recognition.  In Proceedings of the IEEE, 86(11):2278– 2324, November.

X. Li, D. Roth. 2002. Learning Question Classifiers.  In Proceedings of ACL 2002.  

T. Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean.  2013. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS 2013.

T. Nakagawa, K. Inui, S. Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In Proceedings of ACL 2010


B. Pang, L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL 2004.

B. Pang, L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL 2005.

A.S. Razavian, H. Azizpour, J. Sullivan, S. Carlsson 2014. CNN Features off-the-shelf: an Astounding Baseline. CoRR, abs/1403.6382.

Y. Shen, X. He, J. Gao, L. Deng, G. Mesnil. 2014.  Learning Semantic Representations Using Convolutional Neural Networks for Web Search. In Proceedings of WWW 2014.

J. Silva, L. Coheur, A. Mendes, A. Wichert. 2011.  From symbolic to sub-symbolic information in question classification. Artificial Intelligence Review, 35(2):137–154.

R. Socher, J. Pennington, E. Huang, A. Ng, C. Manning.  2011. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of EMNLP 2011.  

R. Socher, B. Huval, C. Manning, A. Ng. 2012. Semantic Compositionality through Recursive MatrixVector Spaces. In Proceedings of EMNLP 2012.

R. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning, A. Ng, C. Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.  In Proceedings of EMNLP 2013.

J. Wiebe, T. Wilson, C. Cardie. 2005. Annotating Expressions of Opinions and Emotions in Language.  Language Resources and Evaluation, 39(2-3): 165– 210.

S. Wang, C. Manning. 2012. Baselines and Bigrams: Simple, Good Sentiment and Topic Classification.  In Proceedings of ACL 2012.

S. Wang, C. Manning. 2013. Fast Dropout Training.  In Proceedings of ICML 2013.

B. Yang, C. Cardie. 2014. Context-aware Learning for Sentence-level Sentiment Analysis with Posterior Regularization. In Proceedings of ACL 2014.  

W. Yih, K. Toutanova, J. Platt, C. Meek. 2011. Learning Discriminative Projections for Text Similarity Measures. Proceedings of the Fifteenth Conference on Computational Natural Language Learning, 247–256.

W. Yih, X. He, C. Meek. 2014. Semantic Parsing for Single-Relation Question Answering. In Proceedings of ACL 2014.

M. Zeiler. 2012. Adadelta: An adaptive learning rate method. CoRR, abs/1212.5701.
